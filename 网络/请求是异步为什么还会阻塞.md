# 请求是异步的为什么还会造成阻塞? 

## 同步和异步
同步和异步关注的是消息通信机制

1. 同步就是在发出一个调用后，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。

2. 异步就是调用发出后，这个调用就直接返回了，所以没有返回结果。换句话说调用者不会立刻得到结果而是在调用返回的时候被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。


## 阻塞和非阻塞
阻塞和非阻塞关注的是程序在等待调用结果时的状态.

阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。

## HTTP 协议的队首阻塞

队首阻塞：队首的事情没有处理完的时候，后面的都要等着

对于长链接来说：同一个 tcp 连接上，下一个请求发出之前需要等到上一个请求的结果返回，所以如果前一个请求的处理时间较长或者是被挂起，那么后面的请求就会一直排队等待。（所以 http 长链接的队头阻塞发生在客户端）

1. 对于 http1.1 的管道化来说：

HTTP1.1 版本上使用了一种 Pipelining 管道技术来并行发送和处理多个请求。让客户端能够并行发送多个请求，服务器端也可以并行处理多个来自客户端的请求。在一个 TCP 连接中，发送多个 HTTP 请求，不需要等待服务器端对前一个请求的响应之后，再发送下一个请求。但是使用了管道技术的 HTTP/1.1，根据 HTTP/1.1 的规则，服务器端在响应时，要严格按照接收请求的顺序发送，即先接收到的请求，需要先发送其响应，客户端浏览器也是如此，接收响应的顺序要按照自己发送请求的顺序来。这样造成的问题是，如果最先收到的请求的处理时间长的话，响应生成也慢，就会阻塞已经生成了的响应的发送，也会造成队首阻塞。

总的来说，管道技术允许客户端和服务器端并行发送多个请求和响应，但是客户端接收响应的顺序要和自己发送请求的顺序对应，服务器端发送响应的顺序要和自己接收到的请求的顺序对应，这样做似乎没什么问题，看起来是不是“FIFO"先来先服务的方式，如果前面收到的一个请求，在服务器端处理的时间很长，生成响应需要很多时间，那么对于后面的已经处理完生成响应的请求来说，它们只能阻塞等待，等待前面的响应发送完后，自己才能被发送出去(即使该请求的响应已经生成)造成了“队首阻塞”问题。可见队首阻塞发生在服务器端，虽然服务器端并行接收了多个请求，也并行处理生成多个响应，但由于要遵守 HTTP/1.1 的规则，先接收到的请求需要先发送响应，造成了阻塞问题。

http1.1 管道化的队头阻塞发生在服务器端（浏览器默认是禁止使用管道化的，所以并发请求的处理方式只能是同时建立多个 tcp 连接，但是这就会收到浏览器对同一域名下并发连接数的限制。现代浏览器默认是不开启 HTTP Pipelining 的）

管道化的定义：一个支持持久连接的客户端可以在一个连接中发送多个请求（不需要等待任意请求的响应）。收到请求的服务器必须按照请求收到的顺序发送响应。

至于标准为什么这么设定，我们可以大概推测一个原因：由于 HTTP/1.1 是个文本协议，同时返回的内容也并不能区分对应于哪个发送的请求，所以顺序必须维持一致。比如你向服务器发送了两个请求 GET /query?q=A 和 GET /query?q=B，服务器返回了两个结果，浏览器是没有办法根据响应结果来判断响应对应于哪一个请求的。

2. HTTP2 队首阻塞

http2 的多路复用解决了 http 层面的队头阻塞。HTTP2 不使用管道化的方式，而是引入了帧、消息和数据流等概念，每个请求/响应被称为消息，一个流代表一个完整的请求或者响应。每个消息都被拆分成若干个帧进行传输，每个帧都分配一个序号。不同流的帧可以乱序发送，同一个流的帧需要顺序发送。每个帧在传输是属于一个数据流，而一个连接上可以存在多个流，每个流都可以承载双向的消息。各个帧在流和连接上独立传输，到达之后在组装成消息，这样就避免了请求/响应阻塞。

当然，即使使用 HTTP2，如果 HTTP2 底层使用的是 TCP 协议，仍可能出现 TCP 队头阻塞。因为 HTTP/2 并没有解决 TCP 的队首阻塞问题，它仅仅是通过多路复用解决了以前 HTTP1.1 管线化请求时的队首阻塞。比如 HTTP/1.1 时代建立一个 TCP 连接，三个请求组成一个队列发出去，服务器接收到这个队列之后会依次响应，一旦前面的请求阻塞，后面的请求就会无法响应。HTTP/2 是通过分帧并且给每个帧打上流的 ID 去避免依次响应的问题，对方接收到帧之后根据 ID 拼接出流，

这样就可以做到乱序响应从而避免请求时的队首阻塞问题。但是 TCP 层面的队首阻塞是 HTTP/2 无法解决的(HTTP 只是应用层协议，TCP 是传输层协议)，TCP 的阻塞问题是因为传输阶段可能会丢包，一旦包就会等待重新发包，阻塞后续传输，这个问题虽然有滑动窗口(Sliding Window)这个方案，但是只能增强抗干扰，并没有彻底解决。

3. tcp 的队首阻塞

TCP 的阻塞问题是因为传输阶段可能会丢包，一旦丢包就会等待重新发包，其后面的数据包即使到达接收端，也不会被处理而是存放在缓冲区，等待这个丢失的包被发送端重新传到接收端，然后再将收到的数据包按顺序进行组装处理。

假设在单个 TCP 连接上发送语义独立的消息，比如说服务器可能发送 3 幅不同的图像供 Web 浏览器显示。为了营造这几幅图像在用户屏幕上并行显示的效果，服务器先发送第一幅图像的一个断片，再发送第二幅图像的一个断片，然后再发送第三幅图像的一个断片；服务器重复这个过程，直到这 3 幅图像全部成功地发送到浏览器为止。要是第一幅图像的某个断片内容的 TCP 分节丢失了，客户端将保持已到达的不按序的所有数据，直到丢失的分节重传成功。这样不仅延缓了第一幅图像数据的递送，也延缓了第二幅和第三幅图像数据的递送。

4. 如何解决 tcp 的对头阻塞

TCP 中的队头阻塞的产生是由 TCP 自身的实现机制决定的，无法避免。想要在应用程序当中避免 TCP 队头阻塞带来的影响，只有舍弃 TCP 协议。

比如 google 推出的 quic 协议，在某种程度上可以说避免了 TCP 中的队头阻塞，因为它根本不使用 TCP 协议，而是在 UDP 协议的基础上实现了可靠传输。而 UDP 是面向数据报的协议，数据报之间不会有阻塞约束。

此外还有一个 SCTP（流控制传输协议），它是和 TCP、UDP 在同一层次的传输协议。SCTP 的多流特性也可以尽可能的避免队头阻塞的情况。

## 总结
从 TCP 队头阻塞和 HTTP 队头阻塞的原因我们可以看到，出现队头阻塞的原因有两个：

1. 独立的消息数据都在一个链路上传输，也就是有一个“队列”。比如 TCP 只有一个流，多个 HTTP 请求共用一个 TCP 连接

2. 队列上传输的数据有严格的顺序约束。比如 TCP 要求数据严格按照序号顺序，HTTP 管道化要求响应严格按照请求顺序返回

所以要避免队头阻塞，就需要从以上两个方面出发，比如 quic 协议不使用 TCP 协议而是使用 UDP 协议，SCTP 协议支持一个连接上存在多个数据流等等。